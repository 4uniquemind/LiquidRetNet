import torch
import torch.nn as nn

# LiquidTimeLayer class
class LiquidTimeLayer(nn.Module):
    def __init__(self, input_dim, hidden_dim, tau=1., dt=0.1):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.tau = tau
        self.dt = dt
        self.linear1 = nn.Linear(input_dim + hidden_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim)

    def forward(self, x, h):
        dxdt = (x - h) / self.tau
        h = h + self.dt * dxdt
        res = self.linear1(torch.cat([h, x], dim=-1))
        h = self.linear2(self.layer_norm(res) + h)
        return h

# RetNet class
class RetNet(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=1)

    def forward(self, h):
        attn_output, _ = self.attention(h, h, h)
        return attn_output

# Combined LiquidRetNet class
class LiquidRetNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, tau=1., dt=0.1):
        super().__init__()
        self.liquid_layers = nn.ModuleList([LiquidTimeLayer(input_dim=input_dim if i==0 else hidden_dim, hidden_dim=hidden_dim, tau=tau, dt=dt) for i in range(num_layers)])
        self.ret_net = RetNet(hidden_dim)
        self.linear = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(p=0.5)
  
    def forward(self, seq, time_stamp):
        h = torch.zeros(seq.size(0), seq.size(1), self.liquid_layers[0].hidden_dim).to(seq.device)
        for liquid_layer in self.liquid_layers:
            h = liquid_layer(seq, h)

        attn_output = self.ret_net(h)
        out = self.dropout(self.linear(attn_output[-1]))
        return out

# Initialize model
model = LiquidRetNet(input_dim=64, hidden_dim=50, output_dim=10, num_layers=2)

# Training data
seq = torch.randn(100, 32, 64).cuda()
time = torch.arange(100).unsqueeze(1).repeat(1,32).float().cuda()

# Model
model = model.cuda()

# Loss function
criterion = nn.CrossEntropyLoss()

# Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)

# Training
epochs = 50
for epoch in range(epochs):
    optimizer.zero_grad()
    output = model(seq, time)
    loss = criterion(output, torch.randint(0, 10, (32,)).cuda())
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f'Epoch: {epoch+1}, Loss: {loss.item()}')

# Save model 
torch.save(model.state_dict(), "./model.pth")

# Load model
model.load_state_dict(torch.load("./model.pth"))
