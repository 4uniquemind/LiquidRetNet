import torch
import torch.nn as nn
import torch.optim as optim
from torchdiffeq import odeint
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence
import os

# Your defined RetainModule, LiquidODENet, and LiquidRetNet classes go here

# Functions for packing and unpacking sequences with variable lengths
def pack_sequence(inputs, lengths, batch_first=True):
    return pack_padded_sequence(inputs, lengths, batch_first)

def unpack_sequence(packed_inputs, batch_first=True):
    return pad_packed_sequence(packed_inputs, batch_first)

# Function for saving model
def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
    torch.save(state, filename)
    if is_best:
        shutil.copyfile(filename, 'model_best.pth.tar')

# DataLoader collate_fn
def collate_fn(batch):
    inputs, labels = zip(*batch)
    lengths = torch.tensor([len(seq) for seq in inputs])
    lengths, idx = lengths.sort(0, descending=True)
    inputs = [inputs[i] for i in idx]
    labels = [labels[i] for i in idx]

    inputs = pad_sequence(inputs, batch_first=True)
    labels = pad_sequence(labels, batch_first=True)

    return pack_sequence(inputs, lengths, batch_first=True), labels
    
# Training and validation function
def train_validate(model, train_loader, val_loader, optimizer, loss_fn, num_epochs):
    for epoch in range(num_epochs):
        model.train()  # set to training mode
        epoch_loss = 0
        for packed_inputs, labels in train_loader:
            outputs = model(packed_inputs)
            loss = loss_fn(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
            optimizer.step()
            epoch_loss += loss.item()
            
        avg_loss = epoch_loss / len(train_loader)
        print(f'Epoch: {epoch + 1}, Loss: {avg_loss:.4f}')
        
        # validate
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for packed_inputs, labels in val_loader:
                outputs = model(packed_inputs)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        print(f'Validation accuracy: {correct / total * 100 :.2f}%')

        # save model
        save_dict = {
            'epoch': epoch + 1,
            'state_dict': model.state_dict(),
            'optimizer' : optimizer.state_dict(),
        }
        save_checkpoint(save_dict, False, filename=os.path.join("checkpoint.pth.tar"))


model = LiquidRetNet(input_dim=10, hidden_dim=20, num_layers=3, num_heads=2, tau=10, A=0.5, decay=0.9)

dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
dataloader_val = DataLoader(dataset_val, batch_size=32, collate_fn=collate_fn)

loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

train_validate(model, dataloader, dataloader_val, optimizer, loss_fn, num_epochs = 10)
